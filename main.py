import torch
import pandas as pd

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)

# =========================
# 1ë¼ë²¨ ì •ì˜
# =========================
label_list = ["O", "NAME", "POSITION", "COMPANY"]
label2id = {l: i for i, l in enumerate(label_list)}
id2label = {i: l for l, i in label2id.items()}


# =========================
# í† í¬ë‚˜ì´ì €
# =========================
MODEL_NAME = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(example):
    tokenized = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=16
    )
    tokenized["label"] = label2id[example["label"]]
    return tokenized

# =========================
# í•™ìŠµ ë°ì´í„°
# =========================
import pandas as pd
from datasets import Dataset

# CSV ì½ê¸° (ë¼ë²¨ ì»¬ëŸ¼ ì—†ìŒ)
df_name = pd.read_csv("train_data_set/name_train_data.csv", header=None, names=["text"])
df_name["label"] = "NAME"

df_company = pd.read_csv("train_data_set/company_train_data.csv", header=None, names=["text"])
df_company["label"] = "COMPANY"

df_position = pd.read_csv("train_data_set/position_train_data.csv", header=None, names=["text"])
df_position["label"] = "POSITION"

df_o = pd.read_csv("train_data_set/other_train_data.csv", header=None, names=["text"])
df_o["label"] = "O"

# í•©ì¹˜ê¸°
df_train = pd.concat([df_name, df_company, df_position, df_o], ignore_index=True)

# Dataset ë³€í™˜
dataset = Dataset.from_pandas(df_train)

# tokenize ì ìš©
dataset = dataset.map(tokenize, batched=False)

# text ì»¬ëŸ¼ ì œê±°, labelì€ ê·¸ëŒ€ë¡œ
dataset = dataset.remove_columns(["text"])
dataset.set_format("torch")


# =========================
# ëª¨ë¸
# =========================
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

# =========================
# í•™ìŠµ ì„¤ì •
# =========================
training_args = TrainingArguments(
    output_dir="./cls_model",
    per_device_train_batch_size=8,
    num_train_epochs=10,
    logging_steps=5,
    save_strategy="no",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer
)

# =========================
# í•™ìŠµ
# =========================
trainer.train()
print("í•™ìŠµ ì™„ë£Œ")

# =========================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# =========================
def predict(text: str):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=16
    )

    with torch.no_grad():
        outputs = model(**inputs)

    pred_id = torch.argmax(outputs.logits, dim=-1).item()
    return id2label[pred_id]

# =========================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# =========================
tests = [
    "í™ê¸¸ë™",
    "ì£¼ì„",
    "ë„¤ì´ë²„",
    "ã…‡ã„´ã…ã„¹ã„´ã…‡ã„¹",
    "ì‚¼ì„±ì „ì",
    "ì°¨ì¥",
    "ì°¨ì€ìš°",
    "êµ¬ë§ˆìœ ì‹œ",
    "ì„ ìš°ì •ì•„",
    "LGì „ì",
    "í•œí™”ì†í•´ë³´í—˜",
    "HP Enterprise",
    "ë¡¯ë°ì •ë³´í†µì‹ ",
    "ì‚¼ì„±SDI",
    "í† ìŠ¤",
]

print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼")
for t in tests:
    print(f"{t} â†’ {predict(t)}")
